2. Spark examples

Spark is a new and quickly developing project. Current version is 2.1.1. If you use Cloudera quickstart image, then you already have Spark. Most probably you will have Spark version 1.5 or 1.6. 

If you use Cloudera-Udacity image, then you don't have Spark and it will be too much hassle to install it. Despite being just 3 or 4 years old, this image is badly outdated.

In general, you don't need Hadoop cluster to run Spark, you can install it even on Windows laptop. It is possible but not always trivial.

Native language for Spark is Scala but we will use Python as it can be safely assumed that everyone knows Python. In fact we are going to use PySpark - Python API for Spark. To start, you just need to run command in terminal window:

[cloudera@quickstart ~]$ pyspark
... and you are in!

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.5.0-cdh5.5.0
      /_/

Using Python version 2.6.6 (r266:84292, Feb 22 2013 00:00:18)
SparkContext available as sc, HiveContext available as sqlContext.
>>> 
You see a welcome note and triple shevron - PySpark (Python) shell introduction. Also, you see that there are two modules already loaded and available: SparkContext as sc, and HiveContext as sqlContext. 

You can use command dir() to look what methods available to you from these modules. Also, majority other Python commands will work too. 

Example in Spark Context
We start with our old friends - movie ratings.

>>> data = sc.textFile("/user/cloudera/prac10/input/users.dat")
... some output
>>> type(data)
<class 'pyspark.rdd.RDD'>
RDD stands for resilient distributed dataset. It is the main abstraction provided by Spark. It is a collection of elements and this collection is partitioned between multiple computers in a cluster.

data.first()   # get first element of RDD
data.take(5)   # get first 5 elements of RDD
data.count()   # count a number of elements in RDD, that is number of users
Here http://spark.apache.org/docs/2.1.0/api/python/pyspark.html you can find all extra commands that you get from PySpark. For example, there is countApprox(timeout, confidence=0.95), that is not exact but approximate count. You get approximate number Spark has managed to get before the timeout. Useful feature while dealing with really big data.

Now, let's calculate average age for males and females as we did before.

temp = data.map(lambda x: x.split("::")[1:3])
Here we split every line of text by a given delimiter and select elements 2 and 3 only - gender and age

temp.collect()
You can have a look at the result but please don't use this command for large files. It is assumed that result of this command is small enough to fit in a memory of your computer (not cluster).

temp = temp.mapValues(lambda v: (v, 1))
It is an equivalent of preparing data in a mapper for sorting and following summation and counting. Yes, we still have to calculate averages manually. Try to use 'collect' to see how your data looks like now.

temp = temp.reduceByKey(lambda a, b: (int(a[0]) + int(b[0]), a[1] + b[1]))
And this is a reducer. We convert first element of the tuple (age) from text to number and do summation. Then we do summation for the second element. As it is just number 1, then it is just counting. Again, use 'temp.collect()' to see results.

temp = temp.mapValues(lambda v: v[0] / float(v[1]))
Final step - devide summation by count and get results. Don't forget to convert at least one element in to float.

temp.collect()
Obviously, result is the same as in previous practicals.

All command above are methods of RDD object, hence we can use them in one line command:

data.map(lambda x: x.split("::")[1:3]).mapValues(lambda v: (v, 1)).reduceByKey(lambda a, b: (int(a[0]) + int(b[0]), a[1] + b[1])).mapValues(lambda v: v[0] / float(v[1])).collect()
... or the same single command in a more readable way:

data.map(lambda x: x.split("::")[1:3]) \
    .mapValues(lambda v: (v, 1)) \
    .reduceByKey(lambda a, b: (int(a[0]) + int(b[0]), a[1] + b[1])) \
    .mapValues(lambda v: v[0] / float(v[1])) \
    .collect()
Example in Hive Context
The same example can be done in a "less Hadoopian" way, something more "normal" for many students. We can use databases approach in PySpark. 

Unfortunately we have a bit outdated version of Spark - it changes so quick, it's hard to keep up. So, we have to use Spark Context to load file and split by delimiter and then convert it in to DataFrame. With Spark version 2.0, we can load CSV file directly by SQL Context.

data = sc.textFile("/user/cloudera/prac10/input/users.dat").map(lambda x: x.split("::"))
df = data.toDF(["usedid", "gender", "age", "job", "postcode"])
And after that we got all power of Data Frames and SQL Context. The same problem of average age for males and females becomes easy:

df.groupBy('gender').agg({'age': 'mean'}).collect()
That's it! Group and then aggregate. 

Please don't disregard SparkContext, it is the main and most powerful tool. SQL Context looks much easier but it is limited to nice (structured) data.
